\documentclass[a4paper]{article}

\input{style}
\newcommand{\courseName}{Deep Learning and Application}
\newcommand{\homeworkID}{Final Project}
\newcommand{\authorName}{Tianyao Chen, Runzhe Yang, Xingyuan Sun}
\newcommand{\authorID}{5140309566, 5140309562, 5140309561}
\newcommand{\semester}{2016-2017 Fall}
\begin{document}



\maketitle
\pagebreak

\section{Introduction}

In this report, we proposed some method for processing data in tone classification problem, used several deep neural networds structure, did some experiments, and compared their performance and time consumptions.

The data set that our teaching assistances gave us has only $400$ data for training. This is a number that is not so sufficient for very large or very deep network to train. Since then, we can only use some small networks. But, the data is not so regular that small networks can not sperate the original data. This situation forces us to find some way to preprocess our data, then to feed the preprocessed data into the networks.

\section{Data Preprocessing}

Our prior knowledge tells us that the tone recognition task only requires pitch contour information, which can be expressed by fundamental frequency. Fortunately, the feature \texttt{f0} as well as \texttt{engy} feature have already given to us. But it does not mean we can use the \texttt{f0} feature directly for three reasons: 1) there are some noise and redundant information in the raw \texttt{f0} feature, which would be great distraction; 2) there numerical values of \texttt{f0} feature varies in a large range so that it will increase difficulties when training; 3) the sequences of features are not of the same length so that it is not suitable for a wide range of models. Therefore, we can not circumvent data preprocessing part. In this section, we will introduce a pipeline of data preprocessing for normalization, removing redundant information, smoothing and denoising data and finally expanding to the same length. Experiments show that data preprocessing can influence performance of models significantly.
\subsection{Mel Scale}
We first convert the foundational frequency (\texttt{f0}) in the mel-scale. The reason we do that is the Mel frequency is much closer the non-linear sensing measurement of human listeners. We use a popular version\footnote{\url{https://en.wikipedia.org/wiki/Mel-frequency_cepstrum}} of mel-scale formula as
	\[\mathtt{Mel}(f) = 2595\log_{10}(1+f/ 700).\]
	
However, the frequencies in mel-scale are not very robust in the presence of additive noise, and so it is common to normalize them in speech recognition systems to lessen the influence of noise.
\subsection{Standardization}
When normalizing data, we only divide its standard derivation but do not subtract its mean, since there are some zeros in the data sequence. Besides, we conduct two types of standardization. One is called \textbf{local standardization}, in which divide the \texttt{std} of a certain feature it self, to avoid some data varies in a huge range. The other is to divide the \texttt{std} of the whole data set for easier training, called \textbf{global standardization}. We do the local standardization first, then do global standardization, for both \texttt{f0} and \texttt{engy}.
\subsection{Removing Redundancy}
As we observe in the training data, there some redundancies in the \texttt{f0} features: Although the \texttt{engy} is very low (even is zero), the \texttt{f0} is still very high and behaves strangely. We think this part of \texttt{f0} is redundant because even human listeners cannot recognize a tone in very low volume. Further, this part of \texttt{f0} is more likely from environments instead of human speaker because of low corresponding \texttt{engy}. Hence, in data preprocessing, we discard the redundancies of \texttt{f0} such that corresponding $\mathtt{standardized(engy)} < 1.0$. 

We only keep the non-zero part of \texttt{f0} after processing as the output of this stage. This part of data still contains some noise. So in the next step, we are going to try to denoise the \texttt{f0} feature.

\subsection{Smoothing}
	We 
%	\cite{tone:2000}

\subsection{Data Expansion}
\begin{enumerate}
\item data.shift

	This dataset is made by shift the beginning of ``f0'' feature to its first non-zero element. Then we pad $0$s at the end of the data to make its length be $160$ (since the longest data has length $157$).

	ToDo: Fix this for data.shift.json

	The dataset is saved in file ``shared data/data.f0.160.json''.
\item data.linear

	ToDo: Description of this dataset.

	The dataset is saved in file ``shared data/data\_linear.json''.

\item data.quad
	
	ToDo: Description of this dataset.

	The dataset is saved in file ``shared data/data\_quad.json''.

\end{enumerate}

\subsection{Centralization}

\section{Models}

We tested two models on the datasets we have made.
\begin{enumerate}
\item pure.fc

	This model has just one fully connected layer. Suppose our input is just $1$-dimensional raw vector $x$, with weight matrix $W$ and bias vector $b$, the scores for the four tones are
	\[
		\text{score} = x \cdot W + b
	\]
	We use softmax loss and l$2$-norm regularization.

\item cnn.fc

	This model has one convolution layer with a 2-strided max-pooling layer followed by one fully connected layer.

\end{enumerate}

\section{Experiments}

We tried each model on each dataset, implemented by each framework.

\subsection{Hyperparameters}

\begin{figure}[H]
\centering
\begin{tabular}{|r|c|c|c|}
\hline
 & value \\
\hline
regularization strength & $0.01$ \\
\hline
batch size & $10$ \\
\hline
optimizer & SGD \\
\hline
learning rate & $3\times10^{-5}$\\
\hline
learning rate decay & N/A \\
\hline
number of epochs & $2000$ \\
\hline
\end{tabular}
\caption{Hyperparameters settings for ``pure.fc'' model.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tabular}{|r|c|c|c|}
\hline
 & value \\
\hline
filter size & $3$ \\
\hline
number of channels & $64$ \\
\hline
regularization strength & $0.002$ \\
\hline
batch size & $10$ \\
\hline
optimizer & SGD \\
\hline
learning rate & $1\times10^{-4}$\\
\hline
learning rate decay & N/A \\
\hline
number of epochs & $400$ \\
\hline
\end{tabular}
\caption{Hyperparameters settings for ``cnn.fc'' model.}
\end{figure}

\subsection{Experiments result}

Now we show the performance and time consumption.

\begin{figure}[H]
\centering
\begin{tabular}{|r|r|c|c|c|}
\hline
 & & data.shift & data.linear & data.quad \\
\hline
pure.fc & training set accuracy & \\
 & test set accuracy & \\
 & test\_new set accuracy & \\
 & time & \\
\hline
cnn.fc & training set accuracy & \\
 & test set accuracy & \\
 & test\_new set accuracy & \\
 & time & \\
\hline
\end{tabular}
\caption{Performance and time consumption on Torch.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tabular}{|r|r|c|c|c|}
\hline
 & & data.shift & data.linear & data.quad \\
\hline
pure.fc & training set accuracy & \\
 & test set accuracy & \\
 & test\_new set accuracy & \\
 & time & \\
\hline
cnn.fc & training set accuracy & \\
 & test set accuracy & \\
 & test\_new set accuracy & \\
 & time & \\
\hline
\end{tabular}
\caption{Performance and time consumption on Theano.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tabular}{|r|r|c|c|c|}
\hline
 & & data.shift & data.linear & data.quad \\
\hline
pure.fc & training set accuracy & $85.25\%$ & $91.00\%$ & $92.75\%$ \\
 & test set accuracy & $80.00\%$ & $100.00\%$ & $100.00\%$ \\
 & test\_new set accuracy & $55.26\%$ & $92.54\%$ & $92.98\%$ \\
 & time & $36.30$s & $35.92$s & $34.87$s \\
\hline
cnn.fc & training set accuracy & $86.25\%$ & $95.00\%$ & $95.75\%$ \\
 & test set accuracy & $77.50\%$ & $100.00\%$ & $100.00\%$ \\
 & test\_new set accuracy & $57.89\%$ & $86.40\%$ & $85.09\%$ \\
 & time & $153.01$s & $127.56$s & $127.99$s \\
\hline
\end{tabular}
\caption{Performance and time consumption on Tensorflow.}
\end{figure}

\section{Conclusion}

\section{References}
\bibliography{main}

\bibliographystyle{main}

\end{document}